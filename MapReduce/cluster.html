<!DOCTYPE html>
<html lang="en">
<head>
	<title>Cluster Architecture | Machine Learning Daily</title>
	<meta name="viewport" content="width=device-width; initial-scale=1.0; maximum-scale=1.0; user-scalable=0;">
	<link rel="stylesheet" href="../css/bootstrap.min.css" type="text/css">
	<link rel="stylesheet" href="../css/normalise.css'?>" type="text/css">
	<link rel="stylesheet" href="../css/style.css'?>" type="text/css">
	<script src="../js/jquery-1.11.3.min.js"></script>
</head>
<body>
	<div class="container-fluid">
		<div class="row">
			<div class="col-md-8 col-md-offset-2" style="text-align:justify">
				<h2><strong>Cluster Architecture</strong></h2><br><hr><br>
				<p class="lead">Let's start with an example.</p>
				<p class="lead">
				Let us say Google has to process about 10 billion pages everyday. The average size of a webpage may be about 20kb amounting to a whole <mark>200 TB</mark> for 10 billion pages. The data of each webpage has to be read from the CPU and the disk read bandwidth averages to about <mark>50 MB/sec</mark>. So, the time taken to read such amount of data by the Google servers would be about <mark>4 million seconds or 46+ days</mark>. So how does Google manage to do it in about 1-2 seconds ?
				</p>
				<p class="lead">
					Now, the obvious thing you can think of is parallel processing of data. This data gets divided into chunks, and chunks are worked upon in parallel and data processing becomes faster cutting down a lot of time. So, if Google has about 1 million servers then it is able to process the faster cutting down the 4 million seconds into 4 seconds. This is the basic idea of <mark>Cluster Architecture</mark>.
				</p>
				<p class="lead">
				Cluster computing itself comes with a number of challenges, like:
				<ul class="list-group">
					<li class="list-group-item lead"><strong>Node Failures,</strong> which check the persistency of each of the nodes. Node failure is possible at any moment and failures during long process computations might cost a lot. In order to remove this, we store the same data on multiple nodes such that copies of the same data are available and computation does not need to stop.</li>
					<li class="list-group-item lead"><strong>Network Bottleneck.</strong> This is a problem with the connection speeds among two nodes or switches. This speed is limited and moving loads of data might take a lot of time.</li>
					<li class="list-group-item lead"><strong>Distributed Programming.</strong> Now distributed programming requires a lot of background knowledge and could be hard to implement. For this we make a simple programming model which hides the complexity underneath.</li>
				</ul>
				</p>
				<p class="lead">
				These challenges are solved by using a <mark>Distributed File System</mark>. A Distributed File System requires writing data once and reading it as many times as is required. Usually it consists of these three components :
				<ul class="list-group">
					<li class="list-group-item lead"><strong>Chunk Servers</strong> serve as computational servers. A chunk is replicated about 2 to 3 times on different servers so as to avoid any loss of data. So, in a way, we are trying to move computation towards data for faster computation.</li>
					<li class="list-group-item lead"><strong>The Master Node</strong>stores the metadata related to each server and assigns them tasks related to computation.</li>
					<li class="list-group-item lead"><strong>Client Library</strong> keeps in contact with the master node to find the chunk server and connects directly to the chunk servers to access the data.</li>
				</ul>
				</p>

			</div>
		</div>
	</div>

</body>
</html>
